# Server configuration
CLASPER_PORT=8081
BACKEND_URL=http://localhost:8000
AGENT_DAEMON_API_KEY=

# Workspace configuration (OpenClaw-inspired portable agent config)
# Path to workspace folder containing AGENTS.md, SOUL.md, etc.
CLASPER_WORKSPACE=./workspace
# Default task title for auto-creation (optional - if not set, requires task_id in request)
CLASPER_DEFAULT_TASK=

# Agent authentication
AGENT_JWT_SECRET=
AGENT_JWT_ALGORITHM=HS256

# Adapter authentication
ADAPTER_JWT_SECRET=
ADAPTER_JWT_ALGORITHM=HS256

# Policy engine
# Path to YAML policy file
CLASPER_POLICY_PATH=./config/policies.yaml
# In OSS there is no approval UI; require_approval would block the agent with no way to unblock.
# allow (default): treat require_approval as allow and audit it. block: block and require override.
CLASPER_REQUIRE_APPROVAL_IN_CORE=allow

# Local ops auth (single-tenant)
# If set, Ops API requires this key via X-Ops-Api-Key
OPS_LOCAL_API_KEY=

# Local scope identifiers (single-tenant)
CLASPER_LOCAL_TENANT_ID=local
CLASPER_LOCAL_WORKSPACE_ID=local

# ===== Multi-Provider LLM Configuration (powered by pi-ai) =====
# Default provider: openai, anthropic, google, xai, groq, mistral, openrouter
LLM_PROVIDER=openai

# Model configuration (format: "model-id" or "provider/model-id")
# If no provider prefix, uses LLM_PROVIDER as default
# Examples:
#   LLM_MODEL_DEFAULT=gpt-4o-mini          (uses LLM_PROVIDER)
#   LLM_MODEL_DEFAULT=anthropic/claude-3-5-sonnet-20241022
#   LLM_MODEL_DEFAULT=google/gemini-2.0-flash
LLM_MODEL_DEFAULT=gpt-4o-mini
LLM_MODEL_CHEAP=
LLM_MODEL_BEST=
LLM_MODEL_FALLBACK=

# ===== Provider API Keys =====
# Only set keys for providers you want to use

# OpenAI (https://platform.openai.com/)
OPENAI_API_KEY=

# Anthropic (https://console.anthropic.com/)
ANTHROPIC_API_KEY=

# Google Gemini (https://ai.google.dev/)
GEMINI_API_KEY=

# xAI Grok (https://x.ai/)
XAI_API_KEY=

# Groq (https://console.groq.com/)
GROQ_API_KEY=

# Mistral (https://console.mistral.ai/)
MISTRAL_API_KEY=

# OpenRouter (https://openrouter.ai/) - access multiple providers with one key
OPENROUTER_API_KEY=

# ===== Legacy OpenAI Config (deprecated, use LLM_* instead) =====
OPENAI_MODEL_DEFAULT=gpt-4o-mini
OPENAI_MODEL_CHEAP=
OPENAI_MODEL_BEST=
OPENAI_MODEL_FALLBACK=

# Retry configuration (OpenClaw-inspired)
# Number of retry attempts for transient failures (rate limits, 5xx, timeouts)
CLASPER_RETRY_ATTEMPTS=3
# Initial delay between retries in milliseconds
CLASPER_RETRY_DELAY_MS=1000
# Maximum delay between retries in milliseconds
CLASPER_RETRY_MAX_DELAY_MS=30000
# Jitter factor (0.1 = 10% randomization to prevent thundering herd)
CLASPER_RETRY_JITTER=0.1

# Context management (OpenClaw-inspired)
# Warn when context usage exceeds this percentage of the model's context window
CLASPER_CONTEXT_WARNING_THRESHOLD=75

# Smart context selection (query-aware skills + memory)
# Enable smart context selection for workspace prompts
CLASPER_SMART_CONTEXT=false
# Max number of skill instructions to include
CLASPER_SMART_CONTEXT_MAX_SKILLS=5
# Max number of memory chunks to include
CLASPER_SMART_CONTEXT_MAX_MEMORY=3
# Optional token budget for smart context (0 = no budget)
CLASPER_SMART_CONTEXT_MAX_TOKENS=0
# Embedding provider: local | openai | none
CLASPER_EMBEDDING_PROVIDER=none
# Embedding model for local or OpenAI provider
CLASPER_EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2

# Time context (OpenClaw-inspired)
# Default timezone for time context injection (empty = use system timezone)
CLASPER_DEFAULT_TIMEZONE=
# Include time context in system prompt (set to "false" to disable)
CLASPER_INCLUDE_TIME_CONTEXT=true

# Internal/dispatcher configuration
INTERNAL_API_TOKEN=
AGENT_DAEMON_URL=http://localhost:8081

# Script defaults (for heartbeat/standup utilities)
USER_ID=
AGENT_ROLE=agent
STANDUP_TIMEZONE=UTC
